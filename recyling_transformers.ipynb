{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8kHb7uHwOEN",
    "outputId": "7dfa7fd5-e6f6-47d4-bb63-adfd9a6eaf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[pytorch] in /opt/conda/lib/python3.8/site-packages (4.15.0)\n",
      "\u001b[33mWARNING: transformers 4.15.0 does not provide the extra 'pytorch'\u001b[0m\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers[pytorch]) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[pytorch]) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers[pytorch]) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[pytorch]) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[pytorch]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[pytorch]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[pytorch]) (2.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers[pytorch]) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers[pytorch]) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers[pytorch]) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers[pytorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working example: https://mccormickml.com/2019/07/22/BERT-fine-tuning/#2-loading-cola-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5iZN97jT9mHJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Sampler, TensorDataset\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Model,\n",
    "    GPT2TokenizerFast,\n",
    ")\n",
    "\n",
    "zipurl = \"https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\"\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall(\"data\")\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/ncullen93/torchsample/blob/master/torchsample/samplers.py#L22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nyu-mll/CoLA-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting embeddings\n",
    "\n",
    "Taken from: https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_acceptability(nn.Module):\n",
    "    def __init__(self, dropout=0.7, model=\"GPT\"):\n",
    "        super(BERT_acceptability, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=True\n",
    "        )\n",
    "        self.ln1 = nn.Linear(76, 50)\n",
    "        self.ln2 = nn.Linear(50, 20)\n",
    "        self.ln3 = nn.Linear(20, 10)\n",
    "        self.ln4 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention):\n",
    "        with torch.no_grad():\n",
    "            out = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention)\n",
    "            hidden_state = out.last_hidden_state\n",
    "            out = torch.mean(hidden_state, dim=1).squeeze()\n",
    "        encoding = nn.functional.max_pool1d(out, 10)\n",
    "        encoding = self.dropout(encoding)\n",
    "        hidden = self.dropout(self.ln1(encoding))\n",
    "        hidden = self.dropout(self.ln2(hidden))\n",
    "        hidden = self.dropout(self.ln3(hidden))\n",
    "        return self.sigmoid(self.ln4(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
      "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51282/3929746227.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/cola_public/raw/in_domain_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"Grammaticality\", \"Empty\", \"Sentence\"],\n",
    ")\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "texts = df.iloc[:, 2].to_list()[0:7000]\n",
    "labels = torch.from_numpy(df.iloc[:, 0].to_numpy()[0:7000])\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader_train = DataLoader(dataset, batch_size=50)\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overtfitting acc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4923, 0: 2077})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032857142857143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(labels.numpy()))\n",
    "\n",
    "4923/7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 10e-4\n",
    "num_epochs = 10\n",
    "\n",
    "model = BERT_acceptability().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = nn.BCELoss()\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5734],\n",
       "        [0.5456],\n",
       "        [0.5683],\n",
       "        [0.5005],\n",
       "        [0.4169],\n",
       "        [0.5477],\n",
       "        [0.2709],\n",
       "        [0.6487],\n",
       "        [0.5320],\n",
       "        [0.5653]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids[0:10].to(device), attention_masks[0:10].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6362857142857142, Mean loss = 0.657236510515213, Correct = 4454/7000\n",
      "Accuracy = 0.6861428571428572, Mean loss = 0.6308093130588531, Correct = 4803/7000\n",
      "Accuracy = 0.696, Mean loss = 0.6251605057290622, Correct = 4872/7000\n",
      "Accuracy = 0.7012857142857143, Mean loss = 0.6181676762444632, Correct = 4909/7000\n",
      "Accuracy = 0.7025714285714286, Mean loss = 0.6187475996358054, Correct = 4918/7000\n",
      "Accuracy = 0.7028571428571428, Mean loss = 0.61243073535817, Correct = 4920/7000\n",
      "Accuracy = 0.7032857142857143, Mean loss = 0.6108057386108807, Correct = 4923/7000\n",
      "Accuracy = 0.7032857142857143, Mean loss = 0.6115283212491445, Correct = 4923/7000\n",
      "Accuracy = 0.7031428571428572, Mean loss = 0.6113501457231385, Correct = 4922/7000\n",
      "Accuracy = 0.7032857142857143, Mean loss = 0.610659750018801, Correct = 4923/7000\n"
     ]
    }
   ],
   "source": [
    "glob_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    losses = []\n",
    "    for ids, attention, y in dataloader_train:\n",
    "        out = model(ids.to(device), attention.to(device))\n",
    "        correct += torch.sum(\n",
    "            torch.round(out).squeeze() == y.squeeze().to(device)\n",
    "        ).item()\n",
    "        loss = loss_fn(out.squeeze(), y.to(torch.float32).squeeze().to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(np.mean(loss.item()))\n",
    "    glob_loss.append(np.mean(losses))\n",
    "    accuracy = correct / len(dataset)\n",
    "    print(\n",
    "        f\"Accuracy = {accuracy}, Mean loss = {np.mean(losses)}, Correct = {correct}/7000\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 47, 0: 3})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6888],\n",
       "        [0.7034],\n",
       "        [0.6802],\n",
       "        [0.6771],\n",
       "        [0.6836],\n",
       "        [0.7182],\n",
       "        [0.7056],\n",
       "        [0.6756],\n",
       "        [0.6693],\n",
       "        [0.6701]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids[0:10].to(device), attention_masks[0:10].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb6d825c0a0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(glob_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6617],\n",
       "        [0.6988],\n",
       "        [0.6812],\n",
       "        [0.6852],\n",
       "        [0.6656],\n",
       "        [0.6532],\n",
       "        [0.6932],\n",
       "        [0.7172],\n",
       "        [0.7045],\n",
       "        [0.6762],\n",
       "        [0.7171],\n",
       "        [0.6865],\n",
       "        [0.6539],\n",
       "        [0.6949],\n",
       "        [0.7218],\n",
       "        [0.6933],\n",
       "        [0.6173],\n",
       "        [0.6769],\n",
       "        [0.6997],\n",
       "        [0.7273]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids[100:120].to(device), attention_masks[100:120].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = df.iloc[:, 2].to_list()[7000:8000]\n",
    "# ids = encode(texts, tokenizer)\n",
    "# # print(len(ids))\n",
    "# labels = torch.from_numpy(df.iloc[:, 0].to_numpy()[7000:8000])\n",
    "# # class_sample_count = [100, 100]\n",
    "# # weights = 1 / torch.Tensor(class_sample_count)\n",
    "# # sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 100)\n",
    "# dataset = TensorDataset(ids, labels)  # create your datset\n",
    "# sampler = StratifiedSampler(class_vector=labels, batch_size=100)\n",
    "# dataloader_test = DataLoader(dataset, batch_size=100, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# print(Counter(labels.numpy()))\n",
    "\n",
    "# 723/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# correct = 0\n",
    "# for ids, y in dataloader_test:\n",
    "#     out = model(ids.to(device))\n",
    "#     correct += torch.sum(torch.round(out).squeeze() == y.squeeze().to(device))\n",
    "#     # print(out.shape)x\n",
    "#     loss = loss_fn(out.squeeze(), y.to(torch.float32).squeeze().to(device))\n",
    "#     losses.append(np.mean(loss.item()))\n",
    "# print(np.mean(losses))\n",
    "# print((correct / 1000).item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "recyling_transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
